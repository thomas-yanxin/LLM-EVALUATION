#è¯¥åº”ç”¨åˆ›å»ºå·¥å…·å…±åŒ…å«ä¸‰ä¸ªåŒºåŸŸï¼Œé¡¶éƒ¨å·¥å…·æ ï¼Œå·¦ä¾§ä»£ç åŒºï¼Œå³ä¾§äº¤äº’æ•ˆæœåŒºï¼Œå…¶ä¸­å³ä¾§äº¤äº’æ•ˆæœæ˜¯é€šè¿‡å·¦ä¾§ä»£ç ç”Ÿæˆçš„ï¼Œå­˜åœ¨å¯¹ç…§å…³ç³»ã€‚
#é¡¶éƒ¨å·¥å…·æ ï¼šè¿è¡Œã€ä¿å­˜ã€æ–°å¼€æµè§ˆå™¨æ‰“å¼€ã€å®æ—¶é¢„è§ˆå¼€å…³ï¼Œé’ˆå¯¹è¿è¡Œå’Œåœ¨æµè§ˆå™¨æ‰“å¼€é€‰é¡¹è¿›è¡Œé‡è¦è¯´æ˜ï¼š
#[è¿è¡Œ]ï¼šäº¤äº’æ•ˆæœå¹¶éå®æ—¶æ›´æ–°ï¼Œä»£ç å˜æ›´åï¼Œéœ€ç‚¹å‡»è¿è¡ŒæŒ‰é’®è·å¾—æœ€æ–°äº¤äº’æ•ˆæœã€‚
#[åœ¨æµè§ˆå™¨æ‰“å¼€]ï¼šæ–°å»ºé¡µé¢æŸ¥çœ‹äº¤äº’æ•ˆæœã€‚
#ä»¥ä¸‹ä¸ºåº”ç”¨åˆ›å»ºå·¥å…·çš„ç¤ºä¾‹ä»£ç 

import os

os.system("python -m pip install paddlepaddle-gpu==0.0.0.post112 -f https://www.paddlepaddle.org.cn/whl/linux/gpu/develop.html")
os.system("pip install --pre --upgrade paddlenlp -f https://www.paddlepaddle.org.cn/whl/paddlenlp.html")
import gradio as gr
# ä½¿ç”¨
from chatglm_paddle import ChatGLM_Predictor
from chatyuan_paddle import ChatYuan_Predictor

chatyuan_predictor = ChatYuan_Predictor()
chatglm_predictor = ChatGLM_Predictor()

batch_size = 1

def batchfy_text(texts, batch_size):
    batch_texts = []
    batch_start = 0
    while batch_start < len(texts):
        batch_texts += [texts[batch_start : min(batch_start + batch_size, len(texts))]]
        batch_start += batch_size
    return batch_texts

def clear_session():
    return '', None, '', None


def chatglm(input, history):
    print(input)
    history = history or []
    if len(history) > 5:
        history = history[-5:]
    all_texts = [text for _, text in history]
    all_texts.append(input)
    print(all_texts)
    batch_texts = batchfy_text(all_texts, batch_size)
    for bs, texts in enumerate(batch_texts):
        outputs = chatglm_predictor.predict(texts)
    print(outputs["result"][0])
    history.append((input, outputs["result"][0]))

    return history, history    
    

def chatyuan(input, history):
    history = history or []
    if len(history) > 5:
        history = history[-5:]

    context = "\n".join([
        f"ç”¨æˆ·ï¼š{input_text}\nå°å…ƒï¼š{answer_text}"
        for input_text, answer_text in history
    ])

    input_text = context + "\nç”¨æˆ·ï¼š" + input + "\nå°å…ƒï¼š"
    output_text = chatyuan_predictor.chatyuan_answer(input_text)
    history.append((input, output_text))

    return history, history


def predict(input, history1, history2):

    history1, history1 = chatyuan(input, history1)

    history2, history2 = chatglm(input, history2)

    print(history1, history2)

    return '', history1, history1, history2, history2


block = gr.Blocks()

with block as demo:
    gr.Markdown("""<h1><center>LLM-EVALUATION</center></h1>

<font size=4>
ğŸ‘ &nbsp; æœ¬é¡¹ç›®æä¾›ä¸€é”®å¼çš„åŸºäºå¤šä¸ªLLMçš„ç”Ÿæˆæ•ˆæœè¯„æµ‹ã€‚æ–¹ä¾¿å¼€å‘è€…ä»LLMçš„ç”Ÿæˆæ•ˆæœè§’åº¦è‡ªæˆ‘è¯„ä»·æ¨¡å‹æ•ˆæœï¼Œä¹Ÿæ–¹ä¾¿é«˜é˜¶å¼€å‘è€…æ›´ä¸ºç›´è§‚ä¸”å‡†ç¡®åœ°åˆ†ææ¨¡å‹åœ¨åœºæ™¯ã€å‚æ•°ç­‰ä¹‹é—´çš„å·®å¼‚ã€‚

ğŸ‘€ &nbsp; ç›®å‰å·²æ”¯æŒChatYuan-large-v2ã€ChatGLM-6Bç­‰æ¨¡å‹ï¼Œåç»­å°†é›†æˆç”Ÿæ€å†…æ›´å¤šæ¨¡å‹ï¼Œè‡´åŠ›äºæ¢ç´¢ä¸­æ–‡é¢†åŸŸå†…çš„ä»¥ç”¨æˆ·ä½“éªŒä¸ºåŸºç¡€çš„LLMæµ‹è¯„æœºåˆ¶ã€‚ç›®å‰ä»æ˜¯é›å½¢ï¼Œæ¬¢è¿å…±å»ºï¼š[Githubåœ°å€](https://github.com/thomas-yanxin/LLM-EVALUATION)

</font>
    """)
    with gr.Row():

        chatbot1 = gr.Chatbot(label='ChatYuan-large-v2')
        chatbot2 = gr.Chatbot(label='ChatGLM-6B')

        state1 = gr.State()
        state2 = gr.State()

    message = gr.Textbox(label='å¯ä»¥åœ¨è¿™é‡Œå¤šå¤šæé—®å—·ï¼')

    message.submit(predict,
                   inputs=[message, state1, state2],
                   outputs=[
                       message, chatbot1, state1, chatbot2, state2, 
                   ])
    with gr.Row():
        clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
        send = gr.Button("ğŸš€ å‘é€")

    send.click(predict,
               inputs=[message, state1, state2],
               outputs=[
                   message, chatbot1, state1, chatbot2, state2
               ])
    clear_history.click(
        fn=clear_session,
        inputs=[],
        outputs=[chatbot1, state1, chatbot2, state2],
        queue=False)

if __name__ == "__main__":
    demo.queue().launch(share=True)

